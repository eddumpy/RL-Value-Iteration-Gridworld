{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL Coursework 1\n",
    "\n",
    "\n",
    "**Date set:** Feb 26, 2018 \n",
    "\n",
    "**Date due:** 8 pm on March 7, 2018 \n",
    "\n",
    "**Total number of marks:** 10 \n",
    "\n",
    "**What to submit:** Completed workbook (.ipynb file)\n",
    "\n",
    "**Where to submit:** CM50270 Moodle page\n",
    "\n",
    "The lab exercises in this workbook will determine 10% of your final mark for CM50270. \n",
    "\n",
    "You need to work on this coursework on your own. You are welcome to discuss it with your classmates but you must write your own code.  \n",
    "\n",
    "Remember to save your work regularly.\n",
    "\n",
    "You must comply with the universities plagiarism guidelines: http://www.bath.ac.uk/library/help/infoguides/plagiarism.html\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "\n",
    "In this coursework, you will implement the Value Iteration algorithm to compute an optimal policy for three different (but related) Markov Decision Processes. For your reference, the pseudo-code for the algorithm is reproduced below from the textbook (Reinforcement Learning, Sutton & Barto, 1998). \n",
    "\n",
    "<img src=\"images/value_iteration.png\" style=\"width: 800px;\"/>\n",
    "\n",
    "Please note the following about the pseudo-code: The set $\\mathcal{S}$ contains all non-terminal states, whereas $\\mathcal{S}^+$ is the set of all states (terminal and non-terminal). The reward $r = r(s, a, s')$ is the expected immediate reward on transition from state $s$ to the next state $s'$ under action $a$. \n",
    "\n",
    "<img src=\"images/bombs and gold numbers.png\" style=\"width: 300px;\" align=\"left\" caption=\"Figure 1\"/>\n",
    "\n",
    "The three problems you will solve use variants of the gridworld environment shown on the left. You should be familiar with this environment from the lectures and from your previous lab exercise. The grid squares in the figure are numbered as shown. In all three problems, the following is true: \n",
    "\n",
    "**Actions available:** The agent has four possible actions in each grid square. These are _west_, _north_, _south_, and _east_. If the direction of movement is blocked by a wall (for example, if the agent executes action south at grid square 1), the agent remains in the same grid square. \n",
    "\n",
    "**Collecting gold:** On its first arrival at a grid square that contains gold, the agent collects the gold. In order to collect the gold, the agent needs to transition into the grid square (containing the gold) from a different grid square. \n",
    "\n",
    "**Hitting the bomb:** On arrival at a grid square that contains the bomb, the agent activates the bomb. \n",
    "\n",
    "** Terminal states:** The game terminates when all gold is collected or when the bomb is activated. In Exercises 1 and 2, you can define terminal states to be grid squares 18 and 23. In Exercise 3, you will need to define terminal state(s) differently.\n",
    "\n",
    "\n",
    "### Instructions ###\n",
    "Set parameter $\\theta$ to 1 to the power of -10. You can express that as `1e-10` in Python. \n",
    "\n",
    "Set all initial state values $V(s)$ to zero.\n",
    "\n",
    "Do not use discounting (that is, set $\\gamma=1$).\n",
    "\n",
    "Use the following reward function: $-1$ for each navigation action (including when the action results in hitting the wall), an additional $+10$ for collecting each piece of gold, and an additional $-10$ for activating the bomb. For example, the immediate reward for transitioning into a square with gold is $-1 + 10 = +9$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Deterministic environment (3 marks)\n",
    "\n",
    "In this first exercise, the agent is able to move in the intended direction with certainty. For example, if it executes action _north_ in grid square 0, it will transition to grid square 5 with probability 1. In other words, we have a deterministic environment. \n",
    "\n",
    "Compute the optimal policy using Value Iteration. \n",
    "\n",
    "Your method needs to output two one-dimensional numpy arrays with names `policy` and `v`. Both arrays should have a length of 25, with the element at index i representing grid cell i. Both arrays should be callable in the \"solution cell\" below! \n",
    "\n",
    "The array `policy` should be an array of strings that specifies an optimal action at each grid location. Please use the abbreviations \"n\", \"e\", \"s\", and \"w\" for the four actions. As an example, policy at index 0 needs to give \"n\", if _north_ is an optimal action in cell 0. The policy for a terminal state can be any value (direction). If there are multiple optimal actions from a state, any optimal action will be considered as a correct answer. \n",
    "\n",
    "The array `v` should be an array of floats that contains the expected return at each grid square (that is, the state value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: 0 \tValue: 3.0 \tPolicy: n\n",
      "State: 1 \tValue: 4.0 \tPolicy: n\n",
      "State: 2 \tValue: 5.0 \tPolicy: n\n",
      "State: 3 \tValue: 4.0 \tPolicy: n\n",
      "State: 4 \tValue: 5.0 \tPolicy: n\n",
      "State: 5 \tValue: 4.0 \tPolicy: n\n",
      "State: 6 \tValue: 5.0 \tPolicy: n\n",
      "State: 7 \tValue: 6.0 \tPolicy: n\n",
      "State: 8 \tValue: 5.0 \tPolicy: n\n",
      "State: 9 \tValue: 6.0 \tPolicy: n\n",
      "State: 10 \tValue: 5.0 \tPolicy: n\n",
      "State: 11 \tValue: 6.0 \tPolicy: n\n",
      "State: 12 \tValue: 7.0 \tPolicy: n\n",
      "State: 13 \tValue: 6.0 \tPolicy: e\n",
      "State: 14 \tValue: 7.0 \tPolicy: n\n",
      "State: 15 \tValue: 6.0 \tPolicy: n\n",
      "State: 16 \tValue: 7.0 \tPolicy: n\n",
      "State: 17 \tValue: 8.0 \tPolicy: n\n",
      "State: 18 \tValue: 0.0 \tPolicy: e\n",
      "State: 19 \tValue: 8.0 \tPolicy: n\n",
      "State: 20 \tValue: 7.0 \tPolicy: e\n",
      "State: 21 \tValue: 8.0 \tPolicy: e\n",
      "State: 22 \tValue: 9.0 \tPolicy: e\n",
      "State: 23 \tValue: 0.0 \tPolicy: e\n",
      "State: 24 \tValue: 9.0 \tPolicy: w\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class Gridworld:\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        self.theta = 1e-10\n",
    "        self.gamma = 1\n",
    "        self.epsilon = 0\n",
    "        self.num_rows = 5\n",
    "        self.num_cols = 5\n",
    "        self.total_cells = self.num_rows * self.num_cols\n",
    "\n",
    "        # Actions available\n",
    "        self.actions = [\"n\", \"e\", \"s\", \"w\"]\n",
    "        self.num_actions = len(self.actions)\n",
    "        \n",
    "        # Default policy array\n",
    "        self.policy = np.array(\n",
    "            [\"n\", \"w\", \"s\", \"w\", \"e\", \"n\", \"w\", \"s\", \"w\", \"e\", \"n\", \"w\", \"s\", \"w\", \"e\", \"n\", \"w\", \"s\", \"e\", \"e\", \"n\",\n",
    "             \"w\", \"s\", \"e\", \"e\"])\n",
    "        \n",
    "    def north(self, state):\n",
    "        north_state = state + self.num_cols\n",
    "        if north_state < self.total_cells:\n",
    "            return north_state\n",
    "        else:\n",
    "            return state\n",
    "\n",
    "    def east(self, state):\n",
    "        east_state = state + 1\n",
    "        if east_state % self.num_cols > 0:\n",
    "            return east_state\n",
    "        else:\n",
    "            return state\n",
    "\n",
    "    def south(self, state):\n",
    "        south_state = state - self.num_cols\n",
    "        if south_state >= 0:\n",
    "            return south_state\n",
    "        else:\n",
    "            return state\n",
    "\n",
    "    def west(self, state):\n",
    "        west_state = state - 1\n",
    "        if west_state % self.num_cols < self.num_cols - 1:\n",
    "            return west_state\n",
    "        else:\n",
    "            return state\n",
    "\n",
    "    def is_terminal(self, state):\n",
    "        if state == 18:\n",
    "            return True\n",
    "        elif state == 23:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "environment = Gridworld()\n",
    "\n",
    "v = np.zeros(environment.total_cells)\n",
    "\n",
    "# Default policy\n",
    "policy = environment.policy\n",
    "theta = environment.theta\n",
    "gamma = environment.gamma\n",
    "actions = environment.actions\n",
    "alpha = 1\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "\n",
    "    delta = 0\n",
    "\n",
    "    for state in range(0, 25):\n",
    "\n",
    "        if not done:\n",
    "\n",
    "            terminal = environment.is_terminal(state)\n",
    "\n",
    "            if not terminal:\n",
    "\n",
    "                # Reward value and current value function of current state\n",
    "                current_value = v[state]\n",
    "\n",
    "                # Gets index of neighbouring states\n",
    "                north_state = environment.north(state)\n",
    "                east_state = environment.east(state)\n",
    "                south_state = environment.south(state)\n",
    "                west_state = environment.west(state)\n",
    "\n",
    "                # Array used to check if any are terminal\n",
    "                state_prime_array = np.array([north_state, east_state, south_state, west_state])\n",
    "                value_list = []\n",
    "\n",
    "                for x in range(0, 4):\n",
    "\n",
    "                    value = 0\n",
    "\n",
    "                    for y in range(0, 4):\n",
    "\n",
    "                        reward = -1\n",
    "\n",
    "                        state_prime = state_prime_array[y]\n",
    "\n",
    "                        if x == y:\n",
    "                            alpha = 1\n",
    "                        else:\n",
    "                            alpha = 0\n",
    "\n",
    "                        if state_prime == 18:\n",
    "                            reward += -10\n",
    "\n",
    "                        if state_prime == 23:\n",
    "                            reward += 10\n",
    "\n",
    "                        value += alpha * (reward + (gamma * v[state_prime]))\n",
    "\n",
    "                    value_list.append(value)\n",
    "                v[state] = max(value_list)\n",
    "                best_action = np.argmax(value_list)\n",
    "\n",
    "                new_value = v[state]\n",
    "\n",
    "                # Updates delta value\n",
    "                delta = max(delta, abs(current_value - new_value))\n",
    "\n",
    "                # Updates policy\n",
    "                policy[state] = actions[best_action]\n",
    "\n",
    "    if delta < theta:\n",
    "        done = True\n",
    "\n",
    "        \n",
    "for i in range(0, len(policy)):\n",
    "    print(\"State:\", i, \"\\tValue:\", v[i], \"\\tPolicy:\", policy[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "dd3738f4ba493ec09092dc7536d7ae52",
     "grade": true,
     "grade_id": "cell-68e298725adc680d",
     "locked": true,
     "points": 3,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Do not delete this cell. Your code for Exercise 1 is tested here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Exercise 2: Stochastic environment (4 marks)\n",
    "\n",
    "In this second exercise, the agent is not always able to execute its actions as intended. With probability 0.8, it moves in the intended direction. With probability 0.2, it moves in a random direction. For example, from grid square 0, if the agent executes action _north_, with probability 0.8, the action will work as intended. But with probability 0.2, the agent's motor control system will move in a random direction (including north). For example, with probability 0.05, it will try to move west (where it will be blocked by the wall and hence remain in grid square 0). Notice that the total probability of moving to square 5 (as intended) is 0.8 + 0.05 = 0.85.\n",
    " \n",
    "Compute the optimal policy using Value Iteration.\n",
    "\n",
    "Your value iteration method should output two one-dimensional numpy arrays with names `policy` and `v`, as in Exercise 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: 0 \tValue: 1.3597920787 \tPolicy: n\n",
      "State: 1 \tValue: 2.1973367201 \tPolicy: n\n",
      "State: 2 \tValue: 2.4287875130 \tPolicy: n\n",
      "State: 3 \tValue: 1.5727216147 \tPolicy: n\n",
      "State: 4 \tValue: 2.5520245101 \tPolicy: n\n",
      "State: 5 \tValue: 2.4869953351 \tPolicy: n\n",
      "State: 6 \tValue: 3.4094598877 \tPolicy: n\n",
      "State: 7 \tValue: 3.6692296713 \tPolicy: n\n",
      "State: 8 \tValue: 2.6412293327 \tPolicy: e\n",
      "State: 9 \tValue: 3.7861011511 \tPolicy: n\n",
      "State: 10 \tValue: 3.6755093765 \tPolicy: n\n",
      "State: 11 \tValue: 4.6962138840 \tPolicy: n\n",
      "State: 12 \tValue: 4.9944186290 \tPolicy: n\n",
      "State: 13 \tValue: 3.2189157994 \tPolicy: e\n",
      "State: 14 \tValue: 5.1025098840 \tPolicy: n\n",
      "State: 15 \tValue: 4.8618511138 \tPolicy: n\n",
      "State: 16 \tValue: 5.9908758698 \tPolicy: n\n",
      "State: 17 \tValue: 6.3708243073 \tPolicy: n\n",
      "State: 18 \tValue: 0.0000000000 \tPolicy: e\n",
      "State: 19 \tValue: 6.4672159320 \tPolicy: n\n",
      "State: 20 \tValue: 6.0416932891 \tPolicy: e\n",
      "State: 21 \tValue: 7.2875663583 \tPolicy: e\n",
      "State: 22 \tValue: 8.6135995087 \tPolicy: e\n",
      "State: 23 \tValue: 0.0000000000 \tPolicy: e\n",
      "State: 24 \tValue: 8.6926231073 \tPolicy: w\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "environment = Gridworld()\n",
    "\n",
    "# V(s) table\n",
    "v = np.zeros(environment.total_cells)\n",
    "\n",
    "# Default policy\n",
    "policy = environment.policy\n",
    "theta = environment.theta\n",
    "gamma = environment.gamma\n",
    "actions = environment.actions\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "\n",
    "    delta = 0\n",
    "\n",
    "    for state in range(0, 25):\n",
    "\n",
    "        if not done:\n",
    "\n",
    "            terminal = environment.is_terminal(state)\n",
    "\n",
    "            if not terminal:\n",
    "\n",
    "                # Reward value and current value function of current state\n",
    "                current_value = v[state]\n",
    "\n",
    "                # Array of adjacent tile states\n",
    "                state_prime_array = np.array([environment.north(state), environment.east(state), environment.south(state), environment.west(state)])\n",
    "                value_list = []\n",
    "\n",
    "                for x in range(0, 4):\n",
    "\n",
    "                    value = 0\n",
    "\n",
    "                    for y in range(0, 4):\n",
    "\n",
    "                        reward = -1\n",
    "                        state_prime = state_prime_array[y]\n",
    "\n",
    "                        if x == y:\n",
    "                            alpha = 0.85\n",
    "                        else:\n",
    "                            alpha = 0.05\n",
    "\n",
    "                        if state_prime == 18:\n",
    "                            reward += -10\n",
    "\n",
    "                        if state_prime == 23:\n",
    "                            reward += 10\n",
    "\n",
    "                        # Adds the sum of the probability to reach chosen new state\n",
    "                        value += alpha * (reward + (gamma * v[state_prime]))\n",
    "\n",
    "                    value_list.append(value)\n",
    "\n",
    "                v[state] = max(value_list)\n",
    "                best_action = np.argmax(value_list)\n",
    "\n",
    "                new_value = v[state]\n",
    "\n",
    "                # Updates delta value\n",
    "                delta = max(delta, abs(current_value - new_value))\n",
    "\n",
    "                # Updates policy\n",
    "                policy[state] = actions[best_action]\n",
    "\n",
    "    if delta < theta:\n",
    "        done = True\n",
    "\n",
    "for i in range(0, len(policy)):\n",
    "    print(\"State:\", i, \"\\tValue: %0.10f\" % v[i], \"\\tPolicy:\", policy[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "79c5cffff4eeffd21c5590981f487de5",
     "grade": true,
     "grade_id": "cell-ff4a8be47847a6a9",
     "locked": true,
     "points": 4,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Do not delete this cell. Your code for Exercise 2 is tested here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Exercise 3: Deterministic environment with two pieces of gold (3 marks)\n",
    "\n",
    "<img src=\"images/bomb and two gold.png\" style=\"width: 300px;\" align=\"left\" caption=\"Figure 1\"/> In this third exercise, the environment is identical to the environment in Exercise 1 with the following exception: there is an additional piece of gold on grid square 12. Recall from earlier instructions that the terminal state is reached only when _all_ gold is collected or when the bomb is activated.\n",
    "\n",
    "Compute the optimal policy using Value Iteration. \n",
    "\n",
    "Hint: You will need to change your state representation. \n",
    "\n",
    "Your method should output two one-dimensional numpy arrays with names `policy` and `v`, as in Exercises 1 and 2. These arrays should specify the expected return and an optimal policy at the corresponding grid sqaure **before any gold is collected or a bomb is activated.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: 0 \tValue: 13.0000000000 \tPolicy: n\n",
      "State: 1 \tValue: 14.0000000000 \tPolicy: n\n",
      "State: 2 \tValue: 15.0000000000 \tPolicy: n\n",
      "State: 3 \tValue: 14.0000000000 \tPolicy: n\n",
      "State: 4 \tValue: 13.0000000000 \tPolicy: n\n",
      "State: 5 \tValue: 14.0000000000 \tPolicy: n\n",
      "State: 6 \tValue: 15.0000000000 \tPolicy: n\n",
      "State: 7 \tValue: 16.0000000000 \tPolicy: n\n",
      "State: 8 \tValue: 15.0000000000 \tPolicy: n\n",
      "State: 9 \tValue: 14.0000000000 \tPolicy: n\n",
      "State: 10 \tValue: 15.0000000000 \tPolicy: e\n",
      "State: 11 \tValue: 16.0000000000 \tPolicy: e\n",
      "State: 12 \tValue: 7.0000000000 \tPolicy: n\n",
      "State: 13 \tValue: 16.0000000000 \tPolicy: w\n",
      "State: 14 \tValue: 15.0000000000 \tPolicy: w\n",
      "State: 15 \tValue: 14.0000000000 \tPolicy: e\n",
      "State: 16 \tValue: 15.0000000000 \tPolicy: e\n",
      "State: 17 \tValue: 16.0000000000 \tPolicy: s\n",
      "State: 18 \tValue: 0.0000000000 \tPolicy: e\n",
      "State: 19 \tValue: 15.0000000000 \tPolicy: n\n",
      "State: 20 \tValue: 14.0000000000 \tPolicy: e\n",
      "State: 21 \tValue: 15.0000000000 \tPolicy: e\n",
      "State: 22 \tValue: 16.0000000000 \tPolicy: e\n",
      "State: 23 \tValue: 7.0000000000 \tPolicy: w\n",
      "State: 24 \tValue: 16.0000000000 \tPolicy: w\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class Gridworld_2:\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        self.theta = 1e-10\n",
    "        self.gamma = 1\n",
    "        self.epsilon = 0\n",
    "        self.num_rows = 5\n",
    "        self.num_cols = 5\n",
    "        self.total_cells = self.num_rows * self.num_cols\n",
    "\n",
    "        # Actions available\n",
    "        self.actions = [\"n\", \"e\", \"s\", \"w\"]\n",
    "        self.num_actions = len(self.actions)\n",
    "\n",
    "        # Default policy array\n",
    "        self.policy = np.array(\n",
    "            [\"n\", \"w\", \"s\", \"w\", \"e\", \"n\", \"w\", \"s\", \"w\", \"e\", \"n\", \"w\", \"s\", \"w\", \"e\", \"n\", \"w\", \"s\", \"e\", \"e\", \"n\",\n",
    "             \"w\", \"s\", \"e\", \"e\", \"n\", \"w\", \"s\", \"w\", \"e\", \"n\", \"w\", \"s\", \"w\", \"e\", \"n\", \"w\", \"s\", \"w\", \"e\", \"n\", \"w\",\n",
    "             \"s\", \"e\", \"e\", \"n\",\n",
    "             \"w\", \"s\", \"e\", \"e\", \"n\", \"w\", \"s\", \"w\", \"e\", \"n\", \"w\", \"s\", \"w\", \"e\", \"n\", \"w\", \"s\", \"w\", \"e\", \"n\", \"w\",\n",
    "             \"s\", \"e\", \"e\", \"n\",\n",
    "             \"w\", \"s\", \"e\", \"e\"])\n",
    "\n",
    "    def north(self, state):\n",
    "\n",
    "        if 0 <= state <= 24:\n",
    "            north_state = state + self.num_cols\n",
    "            if north_state < self.total_cells:\n",
    "                return north_state\n",
    "            else:\n",
    "                return state\n",
    "\n",
    "        elif 25 <= state <= 49:\n",
    "            north_state = state + self.num_cols\n",
    "            if north_state < self.total_cells * 2:\n",
    "                return north_state\n",
    "            else:\n",
    "                return state\n",
    "        else:\n",
    "            north_state = state + self.num_cols\n",
    "            if north_state < self.total_cells * 3:\n",
    "                return north_state\n",
    "            else:\n",
    "                return state\n",
    "\n",
    "    def east(self, state):\n",
    "\n",
    "        east_state = state + 1\n",
    "        if east_state % self.num_cols > 0:\n",
    "            return east_state\n",
    "        else:\n",
    "            return state\n",
    "\n",
    "    def south(self, state):\n",
    "\n",
    "        if 0 <= state <= 24:\n",
    "            south_state = state - self.num_cols\n",
    "            if south_state >= 0:\n",
    "                return south_state\n",
    "            else:\n",
    "                return state\n",
    "\n",
    "        elif 25 <= state <= 49:\n",
    "            south_state = state - self.num_cols\n",
    "            if south_state >= 25:\n",
    "                return south_state\n",
    "            else:\n",
    "                return state\n",
    "\n",
    "        else:\n",
    "            south_state = state - self.num_cols\n",
    "            if south_state >= 50:\n",
    "                return south_state\n",
    "            else:\n",
    "                return state\n",
    "\n",
    "    def west(self, state):\n",
    "        west_state = state - 1\n",
    "        if west_state % self.num_cols < self.num_cols - 1:\n",
    "            return west_state\n",
    "        else:\n",
    "            return state\n",
    "\n",
    "    def is_terminal(self, state):\n",
    "\n",
    "        if state == 18:\n",
    "            return True\n",
    "        elif state == 37:\n",
    "            return True\n",
    "        elif state == 43:\n",
    "            return True\n",
    "        elif state == 68:\n",
    "            return True\n",
    "        elif state == 73:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "environment = Gridworld_2()\n",
    "\n",
    "# V(s) tables\n",
    "v = np.zeros(environment.total_cells * 3)\n",
    "\n",
    "# Default policy\n",
    "policy = environment.policy\n",
    "theta = environment.theta\n",
    "gamma = environment.gamma\n",
    "actions = environment.actions\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "\n",
    "    delta = 0\n",
    "\n",
    "    for state in range(0, 75):\n",
    "\n",
    "        if state == 12:\n",
    "            state = state + 50\n",
    "            special_state = True\n",
    "        elif state == 23:\n",
    "            state = state + 25\n",
    "            special_state = True\n",
    "        else:\n",
    "            special_state = False\n",
    "\n",
    "        # print(\"State: \", state)\n",
    "\n",
    "        if not done:\n",
    "\n",
    "            terminal = environment.is_terminal(state)\n",
    "\n",
    "            if not terminal:\n",
    "\n",
    "                # Reward value and current value function of current state\n",
    "                current_value = v[state]\n",
    "                # print(\"current value: \", current_value)\n",
    "\n",
    "                # Array of adjacent tile states\n",
    "                state_prime_array = np.array(\n",
    "                    [environment.north(state), environment.east(state), environment.south(state),\n",
    "                     environment.west(state)])\n",
    "                # print(\"State: \" + str(state) + \" Moving states: \" + str(state_prime_array))\n",
    "\n",
    "                # Empty list to record state values\n",
    "                value_list = []\n",
    "\n",
    "                for x in range(0, 4):\n",
    "\n",
    "                    value = 0\n",
    "\n",
    "                    for y in range(0, 4):\n",
    "\n",
    "                        reward = -1\n",
    "                        state_prime = state_prime_array[y]\n",
    "\n",
    "                        if x == y:\n",
    "                            alpha = 1\n",
    "                        else:\n",
    "                            alpha = 0\n",
    "\n",
    "                        # Adjust reward for Bomb States\n",
    "                        if state_prime == 18 or state_prime == 43 or state_prime == 68:\n",
    "                            reward += -10\n",
    "\n",
    "                        if state_prime == 12 or state_prime == 23 or state_prime == 37 or state_prime == 73:\n",
    "                            reward += 10\n",
    "\n",
    "                        # Adds the sum of the probability to reach chosen new state\n",
    "                        value += alpha * (reward + (gamma * v[state_prime]))\n",
    "\n",
    "                    value_list.append(value)\n",
    "\n",
    "                if special_state:\n",
    "\n",
    "                    if state == 62:\n",
    "                        v[state - 50] = max(value_list)\n",
    "                        best_action = np.argmax(value_list)\n",
    "                        new_value = v[state - 50]\n",
    "                        policy[state - 50] = actions[best_action]\n",
    "                    else:\n",
    "                        v[state - 25] = max(value_list)\n",
    "                        best_action = np.argmax(value_list)\n",
    "                        new_value = v[state - 25]\n",
    "                        policy[state - 25] = actions[best_action]\n",
    "                else:\n",
    "                    v[state] = max(value_list)\n",
    "                    best_action = np.argmax(value_list)\n",
    "                    new_value = v[state]\n",
    "                    policy[state] = actions[best_action]\n",
    "\n",
    "                # Updates delta value\n",
    "                delta = max(delta, abs(current_value - new_value))\n",
    "    \n",
    "    if delta < theta:\n",
    "        done = True\n",
    "\n",
    "\n",
    "policy = policy[:25]\n",
    "v = v[:25]\n",
    "        \n",
    "for i in range(0, len(policy)):\n",
    "    print(\"State:\", i, \"\\tValue: %0.10f\" % v[i], \"\\tPolicy:\", policy[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "19bba4608a92215d9ab9ef8cb5cd21b6",
     "grade": true,
     "grade_id": "cell-55c5920e89b768cf",
     "locked": true,
     "points": 3,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Do not delete this cell. Your code for Exercise 3 is tested here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
